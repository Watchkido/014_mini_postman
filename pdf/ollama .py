#!/usr/bin/env python3
"""
Ollama Interactive Script - Startet Ollama automatisch und nimmt Benutzereingaben entgegen
Testet die Ollama-Installation und ermÃ¶glicht interaktive Fragen
"""

import subprocess
import os
import time
import signal
import sys
import requests
import json
from pathlib import Path

class OllamaTestManager:
    """
    Verwaltet Ollama-Tests mit automatischem Start und interaktiven Fragen.
    """
    
    def __init__(self):
        self.ollama_executable = None
        self.ollama_process = None
        self.is_running = False
        self.available_models = []
        self.best_model = None
        
        # Signal-Handler fÃ¼r sauberes Beenden
        signal.signal(signal.SIGINT, self._signal_handler)
        if hasattr(signal, 'SIGTERM'):
            signal.signal(signal.SIGTERM, self._signal_handler)
    
    def _signal_handler(self, signum, frame):
        """Signal-Handler fÃ¼r Ctrl+C"""
        print(f"\nğŸ›‘ Signal {signum} empfangen. Beende Ollama...")
        self.stop_ollama()
        sys.exit(0)
    
    def diagnose_ollama(self):
        """
        FÃ¼hrt vollstÃ¤ndige Ollama-Diagnose durch.
        
        :returns: True wenn Ollama funktioniert
        :rtype: bool
        """
        print("ğŸ” Ollama-Diagnose gestartet...\n")
        
        # 1. PrÃ¼fe PATH-Variable
        print("1ï¸âƒ£ PATH-Variable:")
        path_dirs = os.environ.get('PATH', '').split(os.pathsep)
        ollama_in_path = False
        for path_dir in path_dirs:
            if 'ollama' in path_dir.lower():
                print(f"   âœ… Ollama-Pfad gefunden: {path_dir}")
                ollama_in_path = True
        
        if not ollama_in_path:
            print("   âš ï¸ Kein Ollama-Pfad in PATH gefunden")
        print()
        
        # 2. Suche Ollama-Installation
        print("2ï¸âƒ£ Suche Ollama-Installation:")
        possible_paths = [
            "ollama",  # Falls im PATH
            "ollama.exe",
            r"C:\Users\Frank\AppData\Local\Programs\Ollama\ollama.exe",
            r"C:\Program Files\Ollama\ollama.exe",
            r"C:\Program Files (x86)\Ollama\ollama.exe",
            os.path.expanduser(r"~\AppData\Local\Programs\Ollama\ollama.exe"),
        ]
        
        for path in possible_paths:
            if path.startswith('C:') or path.startswith(os.path.expanduser('~')):
                if Path(path).exists():
                    print(f"   âœ… Datei gefunden: {path}")
                else:
                    print(f"   âŒ Datei nicht gefunden: {path}")
                    continue
            
            # Teste AusfÃ¼hrbarkeit
            try:
                result = subprocess.run([path, "--version"], 
                                      capture_output=True, text=True, timeout=10)
                if result.returncode == 0:
                    print(f"   âœ… FUNKTIONIERT: {path}")
                    print(f"      Version: {result.stdout.strip()}")
                    self.ollama_executable = path
                    return True
                else:
                    print(f"   âŒ Exit-Code {result.returncode}: {path}")
                    
            except FileNotFoundError:
                print(f"   âŒ Nicht ausfÃ¼hrbar: {path}")
            except subprocess.TimeoutExpired:
                print(f"   â° Timeout: {path}")
            except Exception as e:
                print(f"   âŒ Fehler: {path} - {e}")
        
        print("\nâŒ PROBLEM: Ollama nicht gefunden oder nicht funktionsfÃ¤hig!")
        print("ğŸ’¡ LÃ¶sungen:")
        print("   1. Installiere Ollama: https://ollama.ai/download")
        print("   2. FÃ¼ge Ollama zum PATH hinzu")
        print("   3. Starte Terminal als Administrator")
        return False
    
    def check_ollama_server(self) -> bool:
        """
        PrÃ¼ft ob Ollama-Server lÃ¤uft.
        
        :returns: True wenn Server erreichbar ist
        :rtype: bool
        """
        try:
            response = requests.get("http://localhost:11434/api/tags", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def get_available_models(self) -> list:
        """
        Holt verfÃ¼gbare Modelle von Ollama.
        
        :returns: Liste der Modellnamen
        :rtype: list
        """
        if not self.ollama_executable:
            return []
        
        try:
            print(f"ğŸ” PrÃ¼fe verfÃ¼gbare Modelle...")
            result = subprocess.run([self.ollama_executable, 'list'], 
                                  capture_output=True, text=True, timeout=15)
            
            if result.returncode == 0:
                lines = result.stdout.strip().split('\n')[1:]  # Header Ã¼berspringen
                models = []
                for line in lines:
                    if line.strip():
                        model_name = line.split()[0]
                        models.append(model_name)
                        print(f"   ğŸ“¦ VerfÃ¼gbares Modell: {model_name}")
                
                self.available_models = models
                return models
            else:
                print(f"âŒ Fehler beim Abrufen der Modelle: {result.stderr}")
                return []
                
        except Exception as e:
            print(f"âŒ Exception: {e}")
            return []
    
    def choose_best_model(self) -> str:
        """
        WÃ¤hlt das beste verfÃ¼gbare Modell aus.
        
        :returns: Name des besten Modells
        :rtype: str
        """
        if not self.available_models:
            return None
        
        # PrioritÃ¤tsliste (beste zuerst)
        priority_models = [
            "qwen2.5-coder:latest",
            "qwen2.5:7b", 
            "deepseek-r1:7b",
            "llama3.2:latest"
        ]
        
        # Suche bestes verfÃ¼gbares Modell
        for preferred in priority_models:
            if preferred in self.available_models:
                print(f"ğŸ¯ Bestes Modell gefunden: {preferred}")
                self.best_model = preferred
                return preferred
        
        # Fallback: Erstes verfÃ¼gbares Modell
        first_model = self.available_models[0]
        print(f"ğŸ¯ Verwende erstes verfÃ¼gbares Modell: {first_model}")
        self.best_model = first_model
        return first_model
    
    def start_ollama_server(self) -> bool:
        """
        Startet Ollama-Server falls nicht lÃ¤uft.
        
        :returns: True wenn Server lÃ¤uft
        :rtype: bool
        """
        if self.check_ollama_server():
            print("âœ… Ollama-Server lÃ¤uft bereits")
            self.is_running = True
            return True
        
        if not self.ollama_executable:
            print("âŒ Ollama-Executable nicht gefunden!")
            return False
        
        if not self.best_model:
            print("âŒ Kein Modell ausgewÃ¤hlt!")
            return False
        
        try:
            print(f"ğŸš€ Starte Ollama-Server mit Modell: {self.best_model}")
            print("   Dies kann einige Minuten dauern...")
            
            # Starte Ollama im Hintergrund
            self.ollama_process = subprocess.Popen(
                [self.ollama_executable, 'serve'],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                creationflags=subprocess.CREATE_NEW_PROCESS_GROUP if os.name == 'nt' else 0
            )
            
            # Warte auf Server-Start
            for i in range(30):
                time.sleep(2)
                if self.check_ollama_server():
                    print(f"âœ… Ollama-Server gestartet (nach {(i+1)*2}s)")
                    self.is_running = True
                    
                    # Lade gewÃ¤hltes Modell
                    return self.load_model(self.best_model)
                
                if i % 5 == 0:
                    print(f"â³ Warte auf Server-Start... ({(i+1)*2}/60s)")
            
            print("âŒ Timeout beim Server-Start")
            return False
            
        except KeyboardInterrupt:
            print(f"\nğŸ›‘ Start durch Ctrl+C abgebrochen")
            self.stop_ollama()
            raise
        except Exception as e:
            print(f"âŒ Fehler beim Starten: {e}")
            return False
    
    def load_model(self, model_name: str) -> bool:
        """
        LÃ¤dt ein spezifisches Modell in Ollama.
        
        :param model_name: Name des zu ladenden Modells
        :type model_name: str
        :returns: True wenn Modell geladen wurde
        :rtype: bool
        """
        try:
            print(f"ğŸ“¥ Lade Modell: {model_name}")
            print("   Dies kann beim ersten Mal lÃ¤nger dauern...")
            
            # FÃ¼hre 'ollama run' aus um Modell zu laden
            result = subprocess.run(
                [self.ollama_executable, 'run', model_name, '--help'],
                capture_output=True, text=True, timeout=120
            )
            
            if result.returncode == 0:
                print(f"âœ… Modell {model_name} erfolgreich geladen")
                return True
            else:
                print(f"âŒ Fehler beim Laden von {model_name}: {result.stderr}")
                return False
                
        except subprocess.TimeoutExpired:
            print(f"â° Timeout beim Laden von {model_name}")
            return False
        except Exception as e:
            print(f"âŒ Exception beim Laden: {e}")
            return False
    
    def ask_question(self, question: str) -> str:
        """
        Stellt eine Frage an Ollama.
        
        :param question: Die zu stellende Frage
        :type question: str
        :returns: Antwort von Ollama
        :rtype: str
        """
        if not self.is_running:
            print("âŒ Ollama-Server lÃ¤uft nicht!")
            return None
        
        if not self.best_model:
            print("âŒ Kein Modell geladen!")
            return None
        
        try:
            print(f"\nğŸ¤– Stelle Frage an {self.best_model}:")
            print(f"â“ Frage: {question}")
            print("â³ Warte auf Antwort...")
            
            # API-Request an Ollama
            payload = {
                "model": self.best_model,
                "prompt": question,
                "stream": False
            }
            
            response = requests.post(
                "http://localhost:11434/api/generate",
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                data = response.json()
                answer = data.get('response', 'Keine Antwort erhalten')
                
                print(f"\nâœ… Antwort erhalten:")
                print(f"ğŸ’¬ {answer}")
                return answer
            else:
                print(f"âŒ HTTP-Fehler {response.status_code}: {response.text}")
                return None
                
        except requests.exceptions.Timeout:
            print("â° Timeout bei der Anfrage")
            return None
        except Exception as e:
            print(f"âŒ Fehler bei der Anfrage: {e}")
            return None
    
    def interactive_session(self):
        """
        Startet eine interaktive Frage-Antwort-Session.
        """
        print(f"\nğŸª Interaktive Ollama-Session gestartet!")
        print(f"ğŸ¤– Aktives Modell: {self.best_model}")
        print("ğŸ“ Gib deine Fragen ein (oder 'quit'/'exit' zum Beenden)")
        print("ğŸ’¡ Befehle: 'help' fÃ¼r Hilfe, 'stats' fÃ¼r Statistiken")
        print("=" * 60)
        
        question_count = 0
        
        while True:
            try:
                # Benutzereingabe holen
                print(f"\nï¿½ Deine Frage #{question_count + 1}: ", end="")
                user_input = input().strip()
                
                # Leer-Eingabe ignorieren
                if not user_input:
                    print("âš ï¸ Leere Eingabe - bitte eine Frage eingeben!")
                    continue
                
                # Beenden-Befehle
                if user_input.lower() in ['quit', 'exit', 'q', 'bye']:
                    print("ğŸ‘‹ Session beendet. Auf Wiedersehen!")
                    break
                
                # Hilfe-Befehl
                elif user_input.lower() in ['help', 'h', '?']:
                    self.show_help()
                    continue
                
                # Statistiken-Befehl
                elif user_input.lower() in ['stats', 'statistics', 'info']:
                    self.show_statistics(question_count)
                    continue
                
                # Modell wechseln
                elif user_input.lower().startswith('model'):
                    self.change_model_interactive()
                    continue
                
                # Frage an Ollama stellen
                else:
                    question_count += 1
                    answer = self.ask_question(user_input)
                    
                    if not answer:
                        print("âš ï¸ Keine Antwort erhalten - versuche es erneut")
                        question_count -= 1  # Fehlgeschlagene Fragen nicht zÃ¤hlen
            
            except KeyboardInterrupt:
                print(f"\nğŸ›‘ Session durch Ctrl+C beendet")
                break
            except EOFError:
                print(f"\nğŸ‘‹ Session beendet")
                break
    
    def show_help(self):
        """Zeigt Hilfe-Informationen."""
        print("\nğŸ“š Ollama Interactive - Hilfe")
        print("=" * 40)
        print("ğŸ”¤ Gib einfach deine Frage ein und drÃ¼cke Enter")
        print("ğŸ“ VerfÃ¼gbare Befehle:")
        print("   â€¢ help, h, ?        - Diese Hilfe anzeigen")
        print("   â€¢ stats, info       - Statistiken anzeigen")
        print("   â€¢ model             - Modell wechseln")
        print("   â€¢ quit, exit, q     - Session beenden")
        print("\nğŸ’¡ Beispiel-Fragen:")
        print("   â€¢ Was ist Python?")
        print("   â€¢ ErklÃ¤re mir Machine Learning")
        print("   â€¢ Schreibe einen Hello World Code")
        print("   â€¢ Was ist die Hauptstadt von Deutschland?")
    
    def show_statistics(self, question_count: int):
        """
        Zeigt Session-Statistiken.
        
        :param question_count: Anzahl gestellter Fragen
        :type question_count: int
        """
        print(f"\nğŸ“Š Ollama Session-Statistiken")
        print("=" * 40)
        print(f"ğŸ¤– Aktives Modell: {self.best_model}")
        print(f"ğŸ”§ Ollama-Pfad: {self.ollama_executable}")
        print(f"ğŸ“¦ VerfÃ¼gbare Modelle: {len(self.available_models)}")
        print(f"â“ Gestellte Fragen: {question_count}")
        print(f"ğŸŒ Server-Status: {'ğŸŸ¢ LÃ¤uft' if self.is_running else 'ğŸ”´ Gestoppt'}")
        
        if self.available_models:
            print(f"\nğŸ“‹ VerfÃ¼gbare Modelle:")
            for i, model in enumerate(self.available_models, 1):
                status = "ğŸ¯ (aktiv)" if model == self.best_model else ""
                print(f"   {i}. {model} {status}")
    
    def change_model_interactive(self):
        """ErmÃ¶glicht interaktiven Modell-Wechsel."""
        if not self.available_models:
            print("âŒ Keine Modelle verfÃ¼gbar!")
            return
        
        print(f"\nï¿½ Modell wechseln")
        print("=" * 30)
        print(f"ğŸ¯ Aktuelles Modell: {self.best_model}")
        print(f"\nğŸ“‹ VerfÃ¼gbare Modelle:")
        
        for i, model in enumerate(self.available_models, 1):
            status = "ğŸ¯ (aktiv)" if model == self.best_model else ""
            print(f"   {i}. {model} {status}")
        
        try:
            print(f"\nğŸ”¢ WÃ¤hle Modell (1-{len(self.available_models)}) oder Enter fÃ¼r aktuelles: ", end="")
            choice = input().strip()
            
            if not choice:  # Enter gedrÃ¼ckt
                print(f"âœ… Behalte aktuelles Modell: {self.best_model}")
                return
            
            model_index = int(choice) - 1
            if 0 <= model_index < len(self.available_models):
                new_model = self.available_models[model_index]
                if new_model != self.best_model:
                    print(f"ğŸ”„ Wechsle zu Modell: {new_model}")
                    if self.load_model(new_model):
                        self.best_model = new_model
                        print(f"âœ… Modell gewechselt zu: {new_model}")
                    else:
                        print(f"âŒ Fehler beim Laden von: {new_model}")
                else:
                    print(f"âœ… Modell {new_model} ist bereits aktiv")
            else:
                print("âŒ UngÃ¼ltige Auswahl!")
                
        except ValueError:
            print("âŒ Bitte eine Zahl eingeben!")
        except Exception as e:
            print(f"âŒ Fehler beim Modell-Wechsel: {e}")
    
    def stop_ollama(self):
        """Stoppt Ollama-Server sauber."""
        if self.ollama_process:
            try:
                print("ğŸ›‘ Stoppe Ollama-Server...")
                if os.name == 'nt':
                    # Windows: Verwende taskkill
                    subprocess.run(['taskkill', '/F', '/T', '/PID', str(self.ollama_process.pid)], 
                                 capture_output=True)
                else:
                    # Unix: Verwende SIGTERM
                    self.ollama_process.terminate()
                    self.ollama_process.wait(timeout=10)
                
                print("âœ… Ollama gestoppt")
            except Exception as e:
                print(f"âš ï¸ Fehler beim Stoppen: {e}")
            finally:
                self.ollama_process = None
                self.is_running = False

def main():
    """Hauptfunktion fÃ¼r interaktive Ollama-Session."""
    ollama_manager = OllamaTestManager()
    
    try:
        print("ğŸš€ Ollama Interactive Script gestartet")
        print("   DrÃ¼cke Ctrl+C zum Beenden\n")
        
        # 1. Diagnose durchfÃ¼hren
        if not ollama_manager.diagnose_ollama():
            print("\nâŒ Ollama-Diagnose fehlgeschlagen!")
            return
        
        # 2. VerfÃ¼gbare Modelle abrufen
        models = ollama_manager.get_available_models()
        if not models:
            print("\nâŒ Keine Modelle verfÃ¼gbar!")
            print("ğŸ’¡ Installiere ein Modell mit: ollama pull qwen2.5:7b")
            return
        
        # 3. Bestes Modell wÃ¤hlen
        best_model = ollama_manager.choose_best_model()
        if not best_model:
            print("\nâŒ Kein geeignetes Modell gefunden!")
            return
        
        # 4. Ollama-Server starten
        if not ollama_manager.start_ollama_server():
            print("\nâŒ Ollama-Server konnte nicht gestartet werden!")
            return
        
        # 5. Interaktive Session starten
        ollama_manager.interactive_session()
    
    except KeyboardInterrupt:
        print(f"\nğŸ›‘ Script durch Benutzer abgebrochen")
    except Exception as e:
        print(f"\nâŒ Unerwarteter Fehler: {e}")
    finally:
        # Cleanup
        ollama_manager.stop_ollama()
        print("\nğŸ‘‹ Interactive Script beendet")

if __name__ == "__main__":
    main()