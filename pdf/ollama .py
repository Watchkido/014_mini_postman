#!/usr/bin/env python3
"""
Ollama Interactive Script - Startet Ollama automatisch und nimmt Benutzereingaben entgegen
Testet die Ollama-Installation und erm√∂glicht interaktive Fragen
"""

import subprocess
import os
import time
import signal
import sys
import requests
import json
from pathlib import Path

class OllamaTestManager:
    """
    Verwaltet Ollama-Tests mit automatischem Start und interaktiven Fragen.
    """
    
    def __init__(self):
        self.ollama_executable = None
        self.ollama_process = None
        self.is_running = False
        self.available_models = []
        self.best_model = None
        
        # Signal-Handler f√ºr sauberes Beenden
        signal.signal(signal.SIGINT, self._signal_handler)
        if hasattr(signal, 'SIGTERM'):
            signal.signal(signal.SIGTERM, self._signal_handler)
    
    def _signal_handler(self, signum, frame):
        """Signal-Handler f√ºr Ctrl+C"""
        print(f"\nüõë Signal {signum} empfangen. Beende Ollama...")
        self.stop_ollama()
        sys.exit(0)
    
    def diagnose_ollama(self):
        """
        F√ºhrt vollst√§ndige Ollama-Diagnose durch.
        
        :returns: True wenn Ollama funktioniert
        :rtype: bool
        """
        print("üîç Ollama-Diagnose gestartet...\n")
        
        # 1. Pr√ºfe PATH-Variable
        print("1Ô∏è‚É£ PATH-Variable:")
        path_dirs = os.environ.get('PATH', '').split(os.pathsep)
        ollama_in_path = False
        for path_dir in path_dirs:
            if 'ollama' in path_dir.lower():
                print(f"   ‚úÖ Ollama-Pfad gefunden: {path_dir}")
                ollama_in_path = True
        
        if not ollama_in_path:
            print("   ‚ö†Ô∏è Kein Ollama-Pfad in PATH gefunden")
        print()
        
        # 2. Suche Ollama-Installation
        print("2Ô∏è‚É£ Suche Ollama-Installation:")
        possible_paths = [
            "ollama",  # Falls im PATH
            "ollama.exe",
            r"C:\Users\Frank\AppData\Local\Programs\Ollama\ollama.exe",
            r"C:\Program Files\Ollama\ollama.exe",
            r"C:\Program Files (x86)\Ollama\ollama.exe",
            os.path.expanduser(r"~\AppData\Local\Programs\Ollama\ollama.exe"),
        ]
        
        for path in possible_paths:
            if path.startswith('C:') or path.startswith(os.path.expanduser('~')):
                if Path(path).exists():
                    print(f"   ‚úÖ Datei gefunden: {path}")
                else:
                    print(f"   ‚ùå Datei nicht gefunden: {path}")
                    continue
            
            # Teste Ausf√ºhrbarkeit
            try:
                result = subprocess.run([path, "--version"], 
                                      capture_output=True, text=True, timeout=10)
                if result.returncode == 0:
                    print(f"   ‚úÖ FUNKTIONIERT: {path}")
                    print(f"      Version: {result.stdout.strip()}")
                    self.ollama_executable = path
                    return True
                else:
                    print(f"   ‚ùå Exit-Code {result.returncode}: {path}")
                    
            except FileNotFoundError:
                print(f"   ‚ùå Nicht ausf√ºhrbar: {path}")
            except subprocess.TimeoutExpired:
                print(f"   ‚è∞ Timeout: {path}")
            except Exception as e:
                print(f"   ‚ùå Fehler: {path} - {e}")
        
        print("\n‚ùå PROBLEM: Ollama nicht gefunden oder nicht funktionsf√§hig!")
        print("üí° L√∂sungen:")
        print("   1. Installiere Ollama: https://ollama.ai/download")
        print("   2. F√ºge Ollama zum PATH hinzu")
        print("   3. Starte Terminal als Administrator")
        return False
    
    def check_ollama_server(self) -> bool:
        """
        Pr√ºft ob Ollama-Server l√§uft.
        
        :returns: True wenn Server erreichbar ist
        :rtype: bool
        """
        try:
            response = requests.get("http://localhost:11434/api/tags", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def get_available_models(self) -> list:
        """
        Holt verf√ºgbare Modelle von Ollama.
        
        :returns: Liste der Modellnamen
        :rtype: list
        """
        if not self.ollama_executable:
            return []
        
        try:
            print(f"üîç Pr√ºfe verf√ºgbare Modelle...")
            result = subprocess.run([self.ollama_executable, 'list'], 
                                  capture_output=True, text=True, timeout=15)
            
            if result.returncode == 0:
                lines = result.stdout.strip().split('\n')[1:]  # Header √ºberspringen
                models = []
                for line in lines:
                    if line.strip():
                        model_name = line.split()[0]
                        models.append(model_name)
                        print(f"   üì¶ Verf√ºgbares Modell: {model_name}")
                
                self.available_models = models
                return models
            else:
                print(f"‚ùå Fehler beim Abrufen der Modelle: {result.stderr}")
                return []
                
        except Exception as e:
            print(f"‚ùå Exception: {e}")
            return []
    
    def choose_best_model(self) -> str:
        """
        W√§hlt das beste verf√ºgbare Modell aus.
        
        :returns: Name des besten Modells
        :rtype: str
        """
        if not self.available_models:
            return None
        
        # Priorit√§tsliste (beste zuerst)
        priority_models = [
            "qwen2.5-coder:latest",
            "qwen2.5:7b", 
            "deepseek-r1:7b",
            "llama3.2:latest"
        ]
        
        # Suche bestes verf√ºgbares Modell
        for preferred in priority_models:
            if preferred in self.available_models:
                print(f"üéØ Bestes Modell gefunden: {preferred}")
                self.best_model = preferred
                return preferred
        
        # Fallback: Erstes verf√ºgbares Modell
        first_model = self.available_models[0]
        print(f"üéØ Verwende erstes verf√ºgbares Modell: {first_model}")
        self.best_model = first_model
        return first_model
    
    def start_ollama_server(self) -> bool:
        """
        Startet Ollama-Server falls nicht l√§uft.
        
        :returns: True wenn Server l√§uft
        :rtype: bool
        """
        if self.check_ollama_server():
            print("‚úÖ Ollama-Server l√§uft bereits")
            self.is_running = True
            return True
        
        if not self.ollama_executable:
            print("‚ùå Ollama-Executable nicht gefunden!")
            return False
        
        if not self.best_model:
            print("‚ùå Kein Modell ausgew√§hlt!")
            return False
        
        try:
            print(f"üöÄ Starte Ollama-Server mit Modell: {self.best_model}")
            print("   Dies kann einige Minuten dauern...")
            
            # Starte Ollama im Hintergrund
            self.ollama_process = subprocess.Popen(
                [self.ollama_executable, 'serve'],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                creationflags=subprocess.CREATE_NEW_PROCESS_GROUP if os.name == 'nt' else 0
            )
            
            # Warte auf Server-Start
            for i in range(30):
                time.sleep(2)
                if self.check_ollama_server():
                    print(f"‚úÖ Ollama-Server gestartet (nach {(i+1)*2}s)")
                    self.is_running = True
                    
                    # Lade gew√§hltes Modell
                    return self.load_model(self.best_model)
                
                if i % 5 == 0:
                    print(f"‚è≥ Warte auf Server-Start... ({(i+1)*2}/60s)")
            
            print("‚ùå Timeout beim Server-Start")
            return False
            
        except KeyboardInterrupt:
            print(f"\nüõë Start durch Ctrl+C abgebrochen")
            self.stop_ollama()
            raise
        except Exception as e:
            print(f"‚ùå Fehler beim Starten: {e}")
            return False
    
    def load_model(self, model_name: str) -> bool:
        """
        L√§dt ein spezifisches Modell in Ollama.
        
        :param model_name: Name des zu ladenden Modells
        :type model_name: str
        :returns: True wenn Modell geladen wurde
        :rtype: bool
        """
        try:
            print(f"üì• Lade Modell: {model_name}")
            print("   Dies kann beim ersten Mal l√§nger dauern...")
            
            # F√ºhre 'ollama run' aus um Modell zu laden
            result = subprocess.run(
                [self.ollama_executable, 'run', model_name, '--help'],
                capture_output=True, text=True, timeout=120
            )
            
            if result.returncode == 0:
                print(f"‚úÖ Modell {model_name} erfolgreich geladen")
                return True
            else:
                print(f"‚ùå Fehler beim Laden von {model_name}: {result.stderr}")
                return False
                
        except subprocess.TimeoutExpired:
            print(f"‚è∞ Timeout beim Laden von {model_name}")
            return False
        except Exception as e:
            print(f"‚ùå Exception beim Laden: {e}")
            return False
    
    def ask_question(self, question: str) -> str:
        """
        Stellt eine Frage an Ollama.
        
        :param question: Die zu stellende Frage
        :type question: str
        :returns: Antwort von Ollama
        :rtype: str
        """
        if not self.is_running:
            print("‚ùå Ollama-Server l√§uft nicht!")
            return None
        
        if not self.best_model:
            print("‚ùå Kein Modell geladen!")
            return None
        
        try:
            print(f"\nü§ñ Stelle Frage an {self.best_model}:")
            print(f"‚ùì Frage: {question}")
            print("‚è≥ Warte auf Antwort...")
            
            # API-Request an Ollama
            payload = {
                "model": self.best_model,
                "prompt": question,
                "stream": False
            }
            
            response = requests.post(
                "http://localhost:11434/api/generate",
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                data = response.json()
                answer = data.get('response', 'Keine Antwort erhalten')
                
                print(f"\n‚úÖ Antwort erhalten:")
                print(f"üí¨ {answer}")
                return answer
            else:
                print(f"‚ùå HTTP-Fehler {response.status_code}: {response.text}")
                return None
                
        except requests.exceptions.Timeout:
            print("‚è∞ Timeout bei der Anfrage")
            return None
        except Exception as e:
            print(f"‚ùå Fehler bei der Anfrage: {e}")
            return None
    
    def interactive_session(self):
        """
        Startet eine interaktive Frage-Antwort-Session.
        """
        print(f"\nüé™ Interaktive Ollama-Session gestartet!")
        print(f"ü§ñ Aktives Modell: {self.best_model}")
        print("üìù Gib deine Fragen ein (oder 'quit'/'exit' zum Beenden)")
        print("üí° Befehle: 'help' f√ºr Hilfe, 'stats' f√ºr Statistiken")
        print("=" * 60)
        
        question_count = 0
        
        while True:
            try:
                # Benutzereingabe holen
                print(f"\nÔøΩ Deine Frage #{question_count + 1}: ", end="")
                user_input = input().strip()
                
                # Leer-Eingabe ignorieren
                if not user_input:
                    print("‚ö†Ô∏è Leere Eingabe - bitte eine Frage eingeben!")
                    continue
                
                # Beenden-Befehle
                if user_input.lower() in ['quit', 'exit', 'q', 'bye']:
                    print("üëã Session beendet. Auf Wiedersehen!")
                    break
                
                # Hilfe-Befehl
                elif user_input.lower() in ['help', 'h', '?']:
                    self.show_help()
                    continue
                
                # Statistiken-Befehl
                elif user_input.lower() in ['stats', 'statistics', 'info']:
                    self.show_statistics(question_count)
                    continue
                
                # Modell wechseln
                elif user_input.lower().startswith('model'):
                    self.change_model_interactive()
                    continue
                
                # Frage an Ollama stellen
                else:
                    question_count += 1
                    answer = self.ask_question(user_input)
                    
                    if not answer:
                        print("‚ö†Ô∏è Keine Antwort erhalten - versuche es erneut")
                        question_count -= 1  # Fehlgeschlagene Fragen nicht z√§hlen
            
            except KeyboardInterrupt:
                print(f"\nüõë Session durch Ctrl+C beendet")
                break
            except EOFError:
                print(f"\nüëã Session beendet")
                break
    
    def show_help(self):
        """Zeigt Hilfe-Informationen."""
        print("\nüìö Ollama Interactive - Hilfe")
        print("=" * 40)
        print("üî§ Gib einfach deine Frage ein und dr√ºcke Enter")
        print("üìù Verf√ºgbare Befehle:")
        print("   ‚Ä¢ help, h, ?        - Diese Hilfe anzeigen")
        print("   ‚Ä¢ stats, info       - Statistiken anzeigen")
        print("   ‚Ä¢ model             - Modell wechseln")
        print("   ‚Ä¢ quit, exit, q     - Session beenden")
        print("\nüí° Beispiel-Fragen:")
        print("   ‚Ä¢ Was ist Python?")
        print("   ‚Ä¢ Erkl√§re mir Machine Learning")
        print("   ‚Ä¢ Schreibe einen Hello World Code")
        print("   ‚Ä¢ Was ist die Hauptstadt von Deutschland?")
    
    def show_statistics(self, question_count: int):
        """
        Zeigt Session-Statistiken.
        
        :param question_count: Anzahl gestellter Fragen
        :type question_count: int
        """
        print(f"\nüìä Ollama Session-Statistiken")
        print("=" * 40)
        print(f"ü§ñ Aktives Modell: {self.best_model}")
        print(f"üîß Ollama-Pfad: {self.ollama_executable}")
        print(f"üì¶ Verf√ºgbare Modelle: {len(self.available_models)}")
        print(f"‚ùì Gestellte Fragen: {question_count}")
        print(f"üåê Server-Status: {'üü¢ L√§uft' if self.is_running else 'üî¥ Gestoppt'}")
        
        if self.available_models:
            print(f"\nüìã Verf√ºgbare Modelle:")
            for i, model in enumerate(self.available_models, 1):
                status = "üéØ (aktiv)" if model == self.best_model else ""
                print(f"   {i}. {model} {status}")
    
    def change_model_interactive(self):
        """Erm√∂glicht interaktiven Modell-Wechsel."""
        if not self.available_models:
            print("‚ùå Keine Modelle verf√ºgbar!")
            return
        
        print(f"\nÔøΩ Modell wechseln")
        print("=" * 30)
        print(f"üéØ Aktuelles Modell: {self.best_model}")
        print(f"\nüìã Verf√ºgbare Modelle:")
        
        for i, model in enumerate(self.available_models, 1):
            status = "üéØ (aktiv)" if model == self.best_model else ""
            print(f"   {i}. {model} {status}")
        
        try:
            print(f"\nüî¢ W√§hle Modell (1-{len(self.available_models)}) oder Enter f√ºr aktuelles: ", end="")
            choice = input().strip()
            
            if not choice:  # Enter gedr√ºckt
                print(f"‚úÖ Behalte aktuelles Modell: {self.best_model}")
                return
            
            model_index = int(choice) - 1
            if 0 <= model_index < len(self.available_models):
                new_model = self.available_models[model_index]
                if new_model != self.best_model:
                    print(f"üîÑ Wechsle zu Modell: {new_model}")
                    if self.load_model(new_model):
                        self.best_model = new_model
                        print(f"‚úÖ Modell gewechselt zu: {new_model}")
                    else:
                        print(f"‚ùå Fehler beim Laden von: {new_model}")
                else:
                    print(f"‚úÖ Modell {new_model} ist bereits aktiv")
            else:
                print("‚ùå Ung√ºltige Auswahl!")
                
        except ValueError:
            print("‚ùå Bitte eine Zahl eingeben!")
        except Exception as e:
            print(f"‚ùå Fehler beim Modell-Wechsel: {e}")
    
    def stop_ollama(self):
        """Stoppt Ollama-Server sauber."""
        if self.ollama_process:
            try:
                print("üõë Stoppe Ollama-Server...")
                if os.name == 'nt':
                    # Windows: Verwende taskkill
                    subprocess.run(['taskkill', '/F', '/T', '/PID', str(self.ollama_process.pid)], 
                                 capture_output=True)
                else:
                    # Unix: Verwende SIGTERM
                    self.ollama_process.terminate()
                    self.ollama_process.wait(timeout=10)
                
                print("‚úÖ Ollama gestoppt")
            except Exception as e:
                print(f"‚ö†Ô∏è Fehler beim Stoppen: {e}")
            finally:
                self.ollama_process = None
                self.is_running = False

def main():
    """Hauptfunktion f√ºr interaktive Ollama-Session."""
    ollama_manager = OllamaTestManager()
    
    try:
        print("üöÄ Ollama Interactive Script gestartet")
        print("   Dr√ºcke Ctrl+C zum Beenden\n")
        
        # 1. Diagnose durchf√ºhren
        if not ollama_manager.diagnose_ollama():
            print("\n‚ùå Ollama-Diagnose fehlgeschlagen!")
            return
        
        # 2. Verf√ºgbare Modelle abrufen
        models = ollama_manager.get_available_models()
        if not models:
            print("\n‚ùå Keine Modelle verf√ºgbar!")
            print("üí° Installiere ein Modell mit: ollama pull qwen2.5:7b")
            return
        
        # 3. Bestes Modell w√§hlen
        best_model = ollama_manager.choose_best_model()
        if not best_model:
            print("\n‚ùå Kein geeignetes Modell gefunden!")
            return
        
        # 4. Ollama-Server starten
        if not ollama_manager.start_ollama_server():
            print("\n‚ùå Ollama-Server konnte nicht gestartet werden!")
            return
        
        # 5. Interaktive Session starten
        ollama_manager.interactive_session()
    
    except KeyboardInterrupt:
        print(f"\nüõë Script durch Benutzer abgebrochen")
    except Exception as e:
        print(f"\n‚ùå Unerwarteter Fehler: {e}")
    finally:
        # Cleanup
        ollama_manager.stop_ollama()
        print("\nüëã Interactive Script beendet")

if __name__ == "__main__":
    main()